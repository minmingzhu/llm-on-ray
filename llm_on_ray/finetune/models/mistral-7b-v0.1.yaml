General:
  base_model: mistralai/Mistral-7B-v0.1
  gpt_base_model: false
  output_dir: /root/llm-on-ray/llm-on-ray/Mistral-7B-lora
  save_strategy: steps
  report_to: wandb
  config:
    trust_remote_code: false
    use_auth_token: null
  lora_config:
    task_type: CAUSAL_LM
    r: 8
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
  enable_gradient_checkpointing: false
Dataset:
  train_file: Open-Orca/SlimOrca
  validation_file: null
  validation_split_percentage: 0
Training:
  optimizer: adamw_torch
  batch_size: 4
  epochs: 2
  learning_rate: 1.0e-04
  lr_scheduler: cosine
  weight_decay: 0.0
  mixed_precision: bf16
  device: hpu
  num_training_workers: 2
  resources_per_worker:
    CPU: 1
    HPU: 1
  accelerate_mode: DEEPSPEED
  gradient_accumulation_steps: 8
  logging_steps: 10
  deepspeed_config_file: llm_on_ray/finetune/ds_config.json
